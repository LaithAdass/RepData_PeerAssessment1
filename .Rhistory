{
correlation
}
for (iterator in data_filter_threshold)
{
data_for_corr_all <- read.csv(full_file_list[iterator])
data_for_corr <- na.omit(data_for_corr_all)
correlation[increment] <- cor(data_for_corr[,2],data_for_corr[,3])
increment <- increment + 1
}
correlation
}
source("complete.R")
complete1 <- function(directory, id = 1:332) {
files_complete <- list.files(directory, full.names = TRUE)
file_c_list <- c(id)
file_value_list <- c()
increment <- 1
for (file in id)
{
place_holder <- read.csv(files_complete[file])
file_value_list[increment] <- nrow(na.omit(place_holder))
increment <- increment + 1
}
data_with_info <- data.frame(id = file_c_list, nobs = file_value_list)
data_with_info
}
cr<-corr("specdata")
cr<- sort(cr)
cr
corr("specdata")
corr("specdata", 129)
corr("specdata", 129)
pollutantmean("specdata", "sulfate", 1:10)
pollutantmean("specdata", "nitrate", 70:72)
pollutantmean("specdata", "sulfate", 34)
pollutantmean("specdata", "nitrate")
cc <- complete("specdata", c(6, 10, 20, 34, 100, 200, 310))
print(cc$nobs)
print(cc)
cc <- complete("specdata", 54)
cc
set.seed(42)
cc <- complete("specdata", 332:1)
use <- sample(332, 10)
print(cc[use])
cc
use
cc<-(complete("specdata", c(304,311,95,274,211,170,44,213,228)))
cc
?print
print(cc[use, "nobs"])
set.seed(42)
cc <- complete("specdata", 332:1)
use <- sample(332, 10)
print(cc[use, "nobs"])
complete <- function(directory, id) {
s <- vector()
for (i in 1:length(id)) {
path <- c(paste(directory, "/",formatC(id[i], width=3, flag=0),".csv",sep=""))
data <- c(read.csv(path))
s[i] <- sum(complete.cases(data))
}
dat <- data.frame(cbind(id,nobs=s))
return(dat)
}
complete2 <- function(directory, id) {
s <- vector()
for (i in 1:length(id)) {
path <- c(paste(directory, "/",formatC(id[i], width=3, flag=0),".csv",sep=""))
data <- c(read.csv(path))
s[i] <- sum(complete.cases(data))
}
dat <- data.frame(cbind(id,nobs=s))
return(dat)
}
set.seed(42)
cc <- complete("specdata", 332:1)
use <- sample(332, 10)
print(cc[use, "nobs"])
cr <- corr("specdata")
cr <- sort(cr)
set.seed(868)
out <- round(cr[sample(length(cr), 5)], 4)
print(out)
source("complete2.R")
corr <- function (directory, threshold = 0)
{
data_frame <- complete(directory)
data_filter_threshold <- which(data_frame$nobs>threshold)
full_file_list <- list.files(directory, full.names=TRUE)
correlation <- c()
increment <- 1
if (length(data_filter_threshold) == 0)
{
correlation
}
for (iterator in data_filter_threshold)
{
data_for_corr_all <- read.csv(full_file_list[iterator])
data_for_corr <- na.omit(data_for_corr_all)
correlation[increment] <- cor(data_for_corr[,2],data_for_corr[,3])
increment <- increment + 1
}
correlation
}
cr <- corr("specdata")
cr <- sort(cr)
set.seed(868)
out <- round(cr[sample(length(cr), 5)], 4)
print(out)
library(datasets)
data(iris)
?iris
?lapply
lapply(iris,mean("Sepal.length"))
lapply(iris,mean())
lapply(x<-iris,mean())
iris
tapply(iris,[,"virginica"],mean(["Sepal.Length"]),simplify=TRUE)
tapply(iris,[,"virginica"],mean(["Sepal.Length"]),simplify=TRUE)
tapply(iris,"virginica",mean(["Sepal.Length"]),simplify=TRUE)
tapply(iris,mean(["Sepal.Length"]),simplify=TRUE)
tapply(iris,mean(),simplify=TRUE)
tapply("iris",mean(),simplify=TRUE)
x<- iris
x
str(tapply)
tapply(x,[,"Sepal.Length"], mean(), simplify=TRUE)
tapply(x,[,"Sepal.Length"], mean(), simplify=TRUE)
tapply(x, mean(), simplify=TRUE)
tapply(x, mean(x), simplify=TRUE)
sapply(split(iris$Sepal.Length, iris$Species), mean)
apply(iris[, 1:4], 2, mean)
library(datasets)
data(mtcars)
?mtcars
sapply(split(mtcars$mpg, mtcars$cyl), mean)
sapply(split(mtcars$hp, mtcars$cyl), mean)
debug(ls)
debug(ls)
q
q
swrilr
Q
source("http://bioconductor.org/bioclite.R")
source("http://bioconductor.org/bioclite.R")
source("https://bioconductor.org/biocLite.R")
biocLite("rhdf5")
library(rhdf5)
created = h5createfile("example.h5")
created = h5createFile("example.h5")
created
created = h5createGroup("example.h5","foo")
created = h5createGroup("example.h5","baa")
created = h5createGroup("example.h5","foo/foobaa")
h5ls("example.h5")
A = matrix(1:10, nr=5,nc=2)
h5write(A,"example.h5","foo/A")
B=array(seq(0,1,2,0, by=0,1),dim=c(5,2,2))
B=array(seq(0,1,2,0,by=0,1),dim=c(5,2,2))
attr(B, "scale")<-"liter"
library(httr)
oauth_endpoints("github")
myapp<-oauth_app("github", key= "16f2ab8c8eb220433d7a", secret="19c4673f6823f74d1a26e7fdada75f6b982054ef" )
github_token<- oauth2.0_token(oauth_endpoints("github"), myapp)
gtoken <- config(token = github_token)
req <- GET("https://api.github.com/rate_limit", gtoken)
stop_for_status(req)
content(req)
tryCatch({
con <- url("http://biostat.jhsph.edu/~jleek/contact.html")
html <- readLines(con)
}, finally = {
close(con)
})
answer4 <- c()
sapply(c(10, 20, 30, 40), function(line) {
answer4 <<- c(answer4, nchar(html[line]))
})
tryCatch({
con <- url("http://biostat.jhsph.edu/~jleek/contact.html")
html <- readLines(con)
}, finally = {
close(con)
})
answer4 <- c()
sapply(c(10, 20, 30, 100), function(line) {
answer4 <<- c(answer4, nchar(html[line]))
})
install.packages("RMySQL")
library(dplyr)
library(dplyr)
library(dplyr)
swril(swirl)
swril
library(swirl)
swril
swirl()
mydf<-read.csv(path2csv, stringsAsFactors = FALSE)
dim(mydf)
head(mydf)
library(dplyr)
packageVersion("dplyr")
cran<-tb1_df(mydf)
cran <- tbl_df(mydf)
rm("mydf")
cran
?select
select( cran ,ip_id,package,country)
5:20
select(cran, r_arch:country)
select(cran, country:r_arch)
cran
select(cran,-time)
select(cran, -x:size)
omnitest(correctExpr='-5:20')
-5:20
-(5:20)
select(cran, -(X:size))
filter(cran, package == "swirl")
filter(cran, r_version == "3.1.1", country == "US")
?Comparison
filter(cran, r_version <= "3.0.2", country == "IN")
filter(cran, country == "US" | country == "IN")
filter(cran, size > 100500, r_os == "linux-gnu")
is.na(c(3, 5, NA, 10))
"!is.na(c(3, 5, NA, 10))"
filter(cran, !is.na(r_version))
cran2 <- select(cran, size:ip_id)
arrange(cran2, ip_id)
!is.na(c(3, 5, NA, 10))
filter(cran, !is.na(r_version))
cran2 <- select(cran, size:ip_id)
arrange(cran2, ip_id)
arrange(cran2, desc(ip_id))
arrange(cran2, package, ip_id)
arrange(cran2, country, desc(r_version), ip_id)
cran3 <- select(cran, ip_id, package, size)
cran3
mutate(cran3, size_mb = size / 2^20)
mutate(cran3, size_mb = size / 2^20, size_gb = size_mb / 2^10)
mutate(cran3, correct_size = size + 1000)
summarize(cran, avg_bytes = mean(size))
library(dplyr)
cran <- tbl_df(mydf)
rm("mydf")
cran
?group_by
by_package <- group_by(cran, package)
by_package
summarize(by_package, mean(size))
submit()
pack_sum
quantile(pack_sum$count, probs = 0.99)
top_counts <- filter(pack_sum, count > 679)
top_counts
View(top_counts)
top_counts_sorted <- arrange(top_counts, desc(count))
View(top_counts)
View(top_counts_sorted)
quantile(pack_sum$unique, probs = 0.99)
top_unique <- filter(pack_sum, unique > 465)
View(top_unique)
top_unique_sorted <- arrange(top_unique, desc(unique))
View(top_unique_sorted)
submit()
submit()
submit()
View(result3)
submit()
submit()
submit()
source('C:/Users/Laith/AppData/Local/Temp/RtmpSo5mVy/chain1.R')
submit()
submit()
submit()
source('C:/Users/Laith/AppData/Local/Temp/RtmpSo5mVy/chain2.R')
submit()
submit()
source('C:/Users/Laith/AppData/Local/Temp/RtmpSo5mVy/chain4.R')
submit()
swirl()
library(tidyr)
students
?gather
gather(students, sex, count, -grade)
students2
res <- gather(students2, sex_class, count, -grade)
res
?seperate
?separate
separate(res, sex_class, c("sex", "class"))
submit()
students3
submit()
students3 %>%
gather( class, grade, class1:class5, na.rm = TRUE) %>%
print
submit()
?spread
source('C:/Users/Laith/AppData/Local/Temp/RtmpSo5mVy/script3.R')
submit()
library(readr)
parse_number("class5")
source('C:/Users/Laith/AppData/Local/Temp/RtmpSo5mVy/script4.R')
submit()
students4
source('C:/Users/Laith/AppData/Local/Temp/RtmpSo5mVy/script5.R')
submit()
source('C:/Users/Laith/AppData/Local/Temp/RtmpSo5mVy/script6.R')
submit()
source('C:/Users/Laith/AppData/Local/Temp/RtmpSo5mVy/script7.R')
submit()
passed
failed
passed <- passed %>% mutate(status = "passed")
failed <- failed %>% mutate(status = "failed")
bind_rows(passed, failed)
sat
source('C:/Users/Laith/AppData/Local/Temp/RtmpSo5mVy/script8.R')
submit()
source('C:/Users/Laith/AppData/Local/Temp/RtmpSo5mVy/script9.R')
submit()
packages <- c("data.table", "xlsx", "XML")
sapply(packages, require, character.only = TRUE, quietly = TRUE)
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FPUMSDataDict06.pdf"
f <- file.path(getwd(), "PUMSDataDict06.pdf")
download.file(url, f, mode = "wb")
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
f <- file.path(getwd(), "ss06hid.csv")
download.file(url, f)
dt <- data.table(read.csv(f))
setkey(dt, VAL)
dt[, .N, key(dt)]
packages <- c("data.table", "jpeg")
sapply(packages, require, character.only = TRUE, quietly = TRUE)
setInternet2(TRUE)
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
f <- file.path(getwd(), "ss06hid.csv")
download.file(url, f)
dt <- data.table(read.csv(f))
agricultureLogical <- dt$ACR == 3 & dt$AGS == 6
which(agricultureLogical)[1:3]
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg"
f <- file.path(getwd(), "jeff.jpg")
download.file(url, f, mode = "wb")
img <- readJPEG(f, native = TRUE)
quantile(img, probs = c(0.3, 0.8))
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
f <- file.path(getwd(), "GDP.csv")
download.file(url, f)
dtGDP <- data.table(read.csv(f, skip = 4, nrows = 215))
dtGDP <- dtGDP[X != ""]
dtGDP <- dtGDP[, list(X, X.1, X.3, X.4)]
setnames(dtGDP, c("X", "X.1", "X.3", "X.4"), c("CountryCode", "rankingGDP",
"Long.Name", "gdp"))
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
f <- file.path(getwd(), "EDSTATS_Country.csv")
download.file(url, f)
dtEd <- data.table(read.csv(f))
dt <- merge(dtGDP, dtEd, all = TRUE, by = c("CountryCode"))
sum(!is.na(unique(dt$rankingGDP)))
dt[order(rankingGDP, decreasing = TRUE), list(CountryCode, Long.Name.x, Long.Name.y,
rankingGDP, gdp)][13]
fname <- "wksst8110.for"
download_if_not_exists(fname, "https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for")
# column sequence: 5x empty space, SST column, SSTA column
col_seq <- c(-5, 4, 4)
# rows: skip first four lines
# cols (left to right):
#   empty space (-1)
#   nine characters (9)
#   etc.
df <- read.fwf(fname,
widths = c(-1, 9, col_seq, col_seq, col_seq, col_seq),
skip = 4)
answer5 <- sum(df[, 4])
swirl()
swirl:swirl()
swirl::swirl()
Sys.getlocale("LC_TIME")
library(lubridate)
help(package="lubridate")
help(package = lubridate)
today()
this_day <-today()
this_day
year(this_day)
wday(this_day)
wday(this_day, label = TRUE)
this_moment<- now()
this_moment
second(this_moment)
my_date <- ymd("1989-05-17")
my_date
class(my_date)
ymd("1989 May 17")
ymd("March 12, 1975")
mdy("March 12, 1975")
dmy("25081985")
dmy(25081985)
ymd("192012")
ymd("1920-1-2")
dt1
ymd_hms(dt1)
hms("03:22:14" )
dt2
ymd("2014-05-14" , "2014-09-22" , "2014-07-11")
ymmd(dt2)
ymd(dt2)
update(this_moment, hours = 8, minutes = 34, seconds = 55)
this_moment
this_moment<-  update(this_moment, hours = 8, minutes = 34, seconds = 55)
this_moment
?now
now(tzone = "America/New_York")
now("America/New_York")
nyc<- now("America/New_York")
nyc
depart<- nyc + days(2)
depart
depart<- update(depart, hours=17, minutes=34)
depart
arrive<- depart + hours(15)+ minutes(50)
?with_tz
with_tz(arrive, tzone= "Asia/Hong_Kong")
with_tz(arrive, "Asia/Hong_Kong")
arrive<- with_tz(arrive, "Asia/Hong_Kong")
arrive
last_time<- mdy("June 17, 2008",tzone="Singapore")
last_time<-mdy("June 17, 2008", tz = "Singapore")
last_time
?interval()
?interval
how_long<- interval(last_time, arrive)
as.period(how_long)
stopwatch()
packages <- c("data.table", "xlsx", "XML")
sapply(packages, require, character.only = TRUE, quietly = TRUE)
packages <- c("data.table", "xlsx", "XML")
sapply(packages, require, character.only = TRUE, quietly = TRUE)
setInternet2(TRUE)
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FPUMSDataDict06.pdf"
f <- file.path(getwd(), "PUMSDataDict06.pdf")
download.file(url, f, mode = "wb")
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
f <- file.path(getwd(), "ss06hid.csv")
download.file(url, f)
dt <- data.table(read.csv(f))
setkey(dt, VAL)
dt[, .N, key(dt)]
library(data.table)
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(fileUrl, destfile = "america_comm_survey.csv")
dateDownloaded <- date()
data <- read.csv("america_comm_survey.csv")
head(data$VAL)
DT = data.table(data)
DT[, .N, by=VAL==24]
dt <- data.table(read.csv(f))
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
f <- file.path(getwd(), "GDP.csv")
download.file(url, f)
dtGDP <- data.table(read.csv(f, skip = 4, nrows = 215, stringsAsFactors = FALSE))
dtGDP <- dtGDP[X != ""]
dtGDP <- dtGDP[, list(X, X.1, X.3, X.4)]
setnames(dtGDP, c("X", "X.1", "X.3", "X.4"), c("CountryCode", "rankingGDP",
"Long.Name", "gdp"))
gdp <- as.numeric(gsub(",", "", dtGDP$gdp))
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
f <- file.path(getwd(), "EDSTATS_Country.csv")
download.file(url, f)
dtEd <- data.table(read.csv(f))
dt <- merge(dtGDP, dtEd, all = TRUE, by = c("CountryCode"))
isFiscalYearEnd <- grepl("fiscal year end", tolower(dt$Special.Notes))
isJune <- grepl("june", tolower(dt$Special.Notes))
table(isFiscalYearEnd, isJune)
source('~/run_analysis.R')
source('~/run_analysis.R')
source('~/run_analysis.R')
source('~/run_analysis.R')
source('~/run_analysis.R')
source('~/run_analysis.R')
source('~/run_analysis.R')
source('~/run_analysis.R')
source('~/run_analysis.R')
tidyMeans
getwd
getwd()
runAnalysis()
1
2
3
getwd()
totStep2<-aggregate(steps~date,data=actFull,sum)
setwd("~/DSCRR-Project1")
unlink('PA1_template_cache', recursive = TRUE)
library(knitr)
install.packages("knitr")
library(knitr)
install.packages("knitr")
library(knitr)
library("knitr")
install.packages("knitr")
library(knitr)
install.packages(c)
install.packages("c")
install.packages("c++")
install.packages("fortran")
install.packages("knitr")
install.packages("knitr")
